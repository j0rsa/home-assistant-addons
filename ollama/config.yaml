---
name: "Ollama"
version: "0.0.1"
slug: "ollama"
description: |
  Ollama 
  
  Runs offine LLM models on your local machine without sending data to the cloud
arch:
  - aarch64
  - amd64
map: 
  - share:rw
codenotary: "red.avtovo@gmil.com"
image: "ghcr.io/j0rsa/ollama-{arch}"
udev: false
url: https://github.com/j0rsa/home-assistant-addons
ports:
  11434/tcp: 11434
ports_description:
  11434/tcp: API port
